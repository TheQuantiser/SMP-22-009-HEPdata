{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb665ab4",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (3565459980.py, line 296)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 296\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mwith open(path, \"r\") as f:\u001b[39m\n                              ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import ROOT\n",
    "from hepdata_lib import Submission, Table, Variable, Uncertainty, RootFileReader\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Basic config (edit these)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "ROOT_FILENAME = \"input/ZNuNuGPrePostFitPostFitEBEE.root\"\n",
    "OUTPUT_DIR = \"hepdata_output\"\n",
    "\n",
    "# Optional: text file containing the full paper abstract (plain text or LaTeX)\n",
    "# If you create such a file, set the path here; otherwise leave as None.\n",
    "ABSTRACT_FILE = None  # e.g. \"abstract_zgamma_smp22009.txt\"\n",
    "\n",
    "OBSERVABLE_NAME = r\"$p_{T}^{\\gamma}$\"\n",
    "OBSERVABLE_UNITS = \"GeV\"\n",
    "\n",
    "USE_GARWOOD = True\n",
    "POISSON_CL = 0.6827  # ~1 sigma\n",
    "\n",
    "CM_ENERGY_GEV = 13000.0  # 13 TeV\n",
    "PAPER_TITLE = (\n",
    "    \"Measurement of the $Z\\\\gamma$ production cross section and search for \"\n",
    "    \"anomalous neutral triple gauge couplings in pp collisions at $\\\\sqrt{s}=13$ TeV\"\n",
    ")\n",
    "CMS_ANALYSIS_ID = \"CMS SMP-22-009\"\n",
    "\n",
    "# Region configuration: mapping from ROOT directories/histograms to table meta\n",
    "REGIONS = [\n",
    "    {\n",
    "        \"name\": \"Figure4_EB_pTgamma\",\n",
    "        \"location\": \"Figure 4 (left)\",\n",
    "        \"description\": (\n",
    "            \"Post-fit reconstruction-level photon transverse momentum $p_{T}^{\\\\gamma}$ \"\n",
    "            \"distribution in the ECAL barrel (EB) signal region for the full Run-2 CMS \"\n",
    "            \"dataset (138 fb$^{-1}$ at $\\\\sqrt{s}=13$ TeV). Black points labelled \"\n",
    "            \"'Observed' show the data with Poisson statistical uncertainties. The \"\n",
    "            \"background expectation is decomposed into the components listed as \"\n",
    "            \"dependent variables in this table. In the published figure, the \"\n",
    "            \"components labelled here as fiducial $Z\\\\gamma$, $W+\\\\gamma$, ECAL \"\n",
    "            \"spikes and misidentified electrons $e\\\\to\\\\gamma$ appear explicitly \"\n",
    "            \"in the legend. The components labelled here as jet fakes, \"\n",
    "            \"out-of-acceptance $Z(\\\\nu\\\\nu)\\\\gamma$ and minor backgrounds are \"\n",
    "            \"drawn in the stack but grouped together and shown as 'Other' in the \"\n",
    "            \"legend. The variable 'Total background' gives the sum of all non-signal \"\n",
    "            \"background components evaluated at their post-fit yields. The variable \"\n",
    "            \"'Predicted' gives the full post-fit model prediction (signal plus all \"\n",
    "            \"background components), with uncertainties propagated from the fit \"\n",
    "            \"covariance.\"\n",
    "        ),\n",
    "        \"root_dir\": \"postfit/EB_SR\",\n",
    "        \"data_hist\": \"data_obs\",\n",
    "        \"primary_mc\": [\n",
    "            (r\"Fiducial $Z\\gamma$\", \"mergedFiducialZNuNuG\"),\n",
    "            (r\"$W+\\gamma$\",         \"mergedWLNuG\"),\n",
    "            (\"Spikes\",              \"ECAL_spikes\"),\n",
    "            (r\"$e\\to\\gamma$\",       \"eleFakes\"),\n",
    "        ],\n",
    "        \"other_mc\": [\n",
    "            (\"Jet fakes\",                         \"jetFakes\"),\n",
    "            (r\"Out-of-acceptance $Z(\\nu\\nu)\\gamma$\", \"ooaFixed\"),\n",
    "            (\"Minor backgrounds\",                 \"minor_bkg\"),\n",
    "        ],\n",
    "        \"extra_mc\": [\n",
    "            (\"Total background\", \"TotalBkg\"),\n",
    "            (\"Predicted\",        \"TotalProcs\"),\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Figure4_EE_pTgamma\",\n",
    "        \"location\": \"Figure 4 (right)\",\n",
    "        \"description\": (\n",
    "            \"Post-fit reconstruction-level photon transverse momentum $p_{T}^{\\\\gamma}$ \"\n",
    "            \"distribution in the ECAL endcap (EE) signal region for the full Run-2 CMS \"\n",
    "            \"dataset (138 fb$^{-1}$ at $\\\\sqrt{s}=13$ TeV). Black points labelled \"\n",
    "            \"'Observed' show the data with Poisson statistical uncertainties. The \"\n",
    "            \"background expectation is decomposed into the components listed as \"\n",
    "            \"dependent variables in this table. In the published figure, the \"\n",
    "            \"components labelled here as fiducial $Z\\\\gamma$, $W+\\\\gamma$, beam \"\n",
    "            \"halo and misidentified electrons $e\\\\to\\\\gamma$ appear explicitly \"\n",
    "            \"in the legend. The components labelled here as jet fakes, \"\n",
    "            \"out-of-acceptance $Z(\\\\nu\\\\nu)\\\\gamma$ and minor backgrounds are \"\n",
    "            \"drawn in the stack but grouped together and shown as 'Other' in the \"\n",
    "            \"legend. The variable 'Total background' gives the sum of all non-signal \"\n",
    "            \"background components evaluated at their post-fit yields. The variable \"\n",
    "            \"'Predicted' gives the full post-fit model prediction (signal plus all \"\n",
    "            \"background components), with uncertainties propagated from the fit \"\n",
    "            \"covariance.\"\n",
    "        ),\n",
    "        \"root_dir\": \"postfit/EE_SR\",\n",
    "        \"data_hist\": \"data_obs\",\n",
    "        \"primary_mc\": [\n",
    "            (r\"Fiducial $Z\\gamma$\", \"mergedFiducialZNuNuG\"),\n",
    "            (r\"$W+\\gamma$\",         \"mergedWLNuG\"),\n",
    "            (r\"$e\\to\\gamma$\",       \"eleFakes\"),\n",
    "            (\"Beam halo\",           \"beamHalo\"),\n",
    "        ],\n",
    "        \"other_mc\": [\n",
    "            (\"Jet fakes\",                         \"jetFakes\"),\n",
    "            (r\"Out-of-acceptance $Z(\\nu\\nu)\\gamma$\", \"OOAfixed\"),\n",
    "            (\"Minor backgrounds\",                 \"minor_bkg\"),\n",
    "        ],\n",
    "        \"extra_mc\": [\n",
    "            (\"Total background\", \"TotalBkg\"),\n",
    "            (\"Predicted\",        \"TotalProcs\"),\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def poisson_garwood_intervals(values, cl=POISSON_CL):\n",
    "    alpha = 1.0 - cl\n",
    "    err_down, err_up = [], []\n",
    "    for y in values:\n",
    "        n = float(y)\n",
    "        if n < 0:\n",
    "            raise ValueError(f\"Negative data bin content: {n}\")\n",
    "        if n == 0.0:\n",
    "            low = 0.0\n",
    "        else:\n",
    "            low = 0.5 * ROOT.Math.chisquared_quantile(alpha / 2.0, 2.0 * n)\n",
    "        up = 0.5 * ROOT.Math.chisquared_quantile_c(alpha / 2.0, 2.0 * (n + 1.0))\n",
    "        err_down.append(n - low)\n",
    "        err_up.append(up - n)\n",
    "    return err_down, err_up\n",
    "\n",
    "\n",
    "def make_mc_variable(label, hist_dict):\n",
    "    v = Variable(label, is_independent=False, is_binned=False)\n",
    "    v.values = hist_dict[\"y\"]\n",
    "    dy = hist_dict.get(\"dy\")\n",
    "    if dy is not None:\n",
    "        if len(dy) > 0 and isinstance(dy[0], tuple):\n",
    "            unc = Uncertainty(\"stat+syst\", is_symmetric=False)\n",
    "            unc.values = dy\n",
    "        else:\n",
    "            unc = Uncertainty(\"stat+syst\", is_symmetric=True)\n",
    "            unc.values = dy\n",
    "        v.add_uncertainty(unc)\n",
    "    return v\n",
    "\n",
    "\n",
    "def make_data_variable(label, hist_dict):\n",
    "    y = hist_dict[\"y\"]\n",
    "    v = Variable(label, is_independent=False, is_binned=False)\n",
    "    v.values = y\n",
    "\n",
    "    if USE_GARWOOD:\n",
    "        err_down, err_up = poisson_garwood_intervals(y)\n",
    "        asym = [(-dn, +up) for dn, up in zip(err_down, err_up)]\n",
    "        unc = Uncertainty(\"stat\", is_symmetric=False)\n",
    "        unc.values = asym\n",
    "    else:\n",
    "        errs = [math.sqrt(val) if val >= 0.0 else 0.0 for val in y]\n",
    "        unc = Uncertainty(\"stat\", is_symmetric=True)\n",
    "        unc.values = errs\n",
    "\n",
    "    v.add_uncertainty(unc)\n",
    "    return v\n",
    "\n",
    "\n",
    "def read_hist_1d(reader, dir_path, hist_name):\n",
    "    \"\"\"Robust path handling for ROOT histograms.\"\"\"\n",
    "    tried = []\n",
    "    path1 = f\"{dir_path}/{hist_name}\" if dir_path else hist_name\n",
    "    tried.append(path1)\n",
    "    try:\n",
    "        return reader.read_hist_1d(path1)\n",
    "    except Exception:\n",
    "        if dir_path and \"/\" in dir_path:\n",
    "            tail = dir_path.split(\"/\", 1)[-1]\n",
    "            path2 = f\"{tail}/{hist_name}\"\n",
    "            tried.append(path2)\n",
    "            try:\n",
    "                return reader.read_hist_1d(path2)\n",
    "            except Exception as e2:\n",
    "                raise RuntimeError(\n",
    "                    f\"Could not find histogram '{hist_name}' in ROOT file. \"\n",
    "                    f\"Tried paths: {tried}\"\n",
    "                ) from e2\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Could not find histogram '{hist_name}' in ROOT file. \"\n",
    "                f\"Tried path: {tried[0]}\"\n",
    "            )\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Build tables and submission\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def build_table_for_region(reader, cfg):\n",
    "    root_dir = cfg[\"root_dir\"]\n",
    "    data_dict = read_hist_1d(reader, root_dir, cfg[\"data_hist\"])\n",
    "\n",
    "    x_var = Variable(\n",
    "        OBSERVABLE_NAME,\n",
    "        is_independent=True,\n",
    "        is_binned=True,\n",
    "        units=OBSERVABLE_UNITS,\n",
    "    )\n",
    "    x_var.values = data_dict[\"x_edges\"]\n",
    "\n",
    "    table = Table(cfg[\"name\"])\n",
    "    table.location = cfg[\"location\"]\n",
    "    table.description = cfg[\"description\"]\n",
    "\n",
    "    # Keywords: common for both regions\n",
    "    table.keywords[\"cmenergies\"] = [CM_ENERGY_GEV]\n",
    "    table.keywords[\"reactions\"] = [\"P P --> Z GAMMA\"]\n",
    "    table.keywords[\"observables\"] = [\"DSIG/DPT\"]\n",
    "    table.keywords[\"phrases\"] = [\n",
    "        \"Differential cross section\",\n",
    "        \"Z to invisible\",\n",
    "        \"High-pt photon\",\n",
    "        \"Run 2\",\n",
    "    ]\n",
    "\n",
    "    table.add_variable(x_var)\n",
    "    table.add_variable(make_data_variable(\"Observed\", data_dict))\n",
    "\n",
    "    for label, hist_name in cfg.get(\"primary_mc\", []):\n",
    "        mc_dict = read_hist_1d(reader, root_dir, hist_name)\n",
    "        table.add_variable(make_mc_variable(label, mc_dict))\n",
    "\n",
    "    for label, hist_name in cfg.get(\"other_mc\", []):\n",
    "        mc_dict = read_hist_1d(reader, root_dir, hist_name)\n",
    "        table.add_variable(make_mc_variable(label, mc_dict))\n",
    "\n",
    "    for label, hist_name in cfg.get(\"extra_mc\", []):\n",
    "        extra_dict = read_hist_1d(reader, root_dir, hist_name)\n",
    "        table.add_variable(make_mc_variable(label, extra_dict))\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def build_submission():\n",
    "    sub = Submission()\n",
    "\n",
    "    # Short top-level description of the analysis\n",
    "    sub.comment = (\n",
    "        f\"{PAPER_TITLE}. CMS analysis {CMS_ANALYSIS_ID}. \"\n",
    "        \"Z→νν with an associated high-$p_{T}$ photon, using 138 fb$^{-1}$ \"\n",
    "        \"of pp collisions at $\\\\sqrt{s}=13$ TeV. \"\n",
    "        \"The tables in this submission provide the post-fit reconstruction-level \"\n",
    "        \"$p_{T}^{\\\\gamma}$ spectra in the EB and EE signal regions corresponding \"\n",
    "        \"to Figure 4 of the paper.\"\n",
    "    )\n",
    "\n",
    "    # Optional abstract file (if you provide one)\n",
    "    if ABSTRACT_FILE is not None and os.path.exists(ABSTRACT_FILE):\n",
    "        sub.read_abstract(ABSTRACT_FILE)\n",
    "\n",
    "    reader = RootFileReader(ROOT_FILENAME)\n",
    "    for cfg in REGIONS:\n",
    "        table = build_table_for_region(reader, cfg)\n",
    "        sub.add_table(table)\n",
    "    return sub\n",
    "\n",
    "\n",
    "def main():\n",
    "    sub = build_submission()\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    sub.create_files(OUTPUT_DIR)\n",
    "    print(f\"HEPData submission written to: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Simple YAML readback/validation\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def validate_hepdata_output(output_dir=OUTPUT_DIR):\n",
    "    \"\"\"Basic sanity checks on generated YAML (bin counts vs. values).\"\"\"\n",
    "    try:\n",
    "        import yaml\n",
    "    except ImportError:\n",
    "        print(\"PyYAML is not available; cannot parse YAML for validation.\")\n",
    "        return\n",
    "\n",
    "    yaml_files = sorted(glob.glob(os.path.join(output_dir, \"*.yaml\")))\n",
    "    if not yaml_files:\n",
    "        print(f\"No YAML files found in {output_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nValidation of HEPData YAML files in: {output_dir}\")\n",
    "    for path in yaml_files:\n",
    "        with open(path, \"r\") as f:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6a1fc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEPData submission written to: hepdata_output\n",
      "\n",
      "Validation of HEPData YAML files in: hepdata_output\n",
      "\n",
      "=== figure4_eb_ptgamma.yaml ===\n",
      "  independent_variables: 1, dependent_variables: 10\n",
      "  number of bins (from first independent variable): 11\n",
      "    Observed: 11 values -> OK\n",
      "    Fiducial $Z\\gamma$: 11 values -> OK\n",
      "    $W+\\gamma$: 11 values -> OK\n",
      "    Spikes: 11 values -> OK\n",
      "    $e\\to\\gamma$: 11 values -> OK\n",
      "    Jet fakes: 11 values -> OK\n",
      "    Out-of-acceptance $Z(\\nu\\nu)\\gamma$: 11 values -> OK\n",
      "    Minor backgrounds: 11 values -> OK\n",
      "    Total background: 11 values -> OK\n",
      "    Predicted: 11 values -> OK\n",
      "\n",
      "=== figure4_ee_ptgamma.yaml ===\n",
      "  independent_variables: 1, dependent_variables: 10\n",
      "  number of bins (from first independent variable): 7\n",
      "    Observed: 7 values -> OK\n",
      "    Fiducial $Z\\gamma$: 7 values -> OK\n",
      "    $W+\\gamma$: 7 values -> OK\n",
      "    $e\\to\\gamma$: 7 values -> OK\n",
      "    Beam halo: 7 values -> OK\n",
      "    Jet fakes: 7 values -> OK\n",
      "    Out-of-acceptance $Z(\\nu\\nu)\\gamma$: 7 values -> OK\n",
      "    Minor backgrounds: 7 values -> OK\n",
      "    Total background: 7 values -> OK\n",
      "    Predicted: 7 values -> OK\n",
      "\n",
      "=== submission.yaml ===\n",
      "  submission.yaml contains 3 document(s).\n",
      "    doc 0: keys = ['additional_resources', 'comment', 'data_license']\n",
      "    doc 1: keys = ['data_file', 'description', 'keywords', 'location', 'name']\n",
      "    doc 2: keys = ['data_file', 'description', 'keywords', 'location', 'name']\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "validate_hepdata_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f6ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combine-634",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
